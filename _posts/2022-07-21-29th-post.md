---
title:  "CrossEntropyLoss vs NLLLoss ?"
excerpt: ""
toc: true
toc_sticky: true

categories:
  - loss
tags:
  - loss, pytorch
last_modified_at: 2022-07-21-08:00
---

## torch.nn.NLLLoss

- [torch.nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)
- **LogSoftmax layer 이후에 사용**
- logsoftmax + nll_loss
- `(y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()`
  - 실제 one_hot vector에서 1인 부분만 계산됨.

## CrossEntropyLoss

- [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

- Note that this case is equivalent to the combination of [LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax) and [NLLLOSS](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss).

- 원래는 log_softmax를 취하지 않는 raw값이 들어가야하지만,  
  
  **log_softmax(log_softmax(x)) = log_softmax(x) 이기 때문에 어떤 값이 들어와도 상관없다.**
