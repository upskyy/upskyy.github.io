---
title:  "MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models"
excerpt: ""
toc: true
toc_sticky: true

categories:
  - llm
tags:
  - llm, multi modal
last_modified_at: 2023-04-05-08:00
---
   
[MiniGPT-4 GitHub](https://github.com/Vision-CAIR/MiniGPT-4)  
[MiniGPT-4 Demo](https://minigpt-4.github.io/)

<br>

## 정리

![image](https://github.com/upskyy/upskyy.github.io/assets/54731898/2a6e1154-fb1a-4cba-a417-c0eab554f6d3)

- GPT-4에서 시연된 것과 유사한 새로운 비전-언어 기능을 제공
  - 이미지에 대해 자세히 설명하고, 음식사진으로 요리방법을 알려주거나, 문제점을 찾거나, 광고문구를 만들거나, 이미지에서 영감받은 이야기와 시를 작성
  
- 단 하나의 프로젝션 레이어를 이용하여 BLIP-2 와 Vicuna(llama를 finetuning한 모델)를 연결한 것만으로 뛰어난 성능을 보여주었다.
  
- 2단계로 훈련했다.
  - 1단계
    - 5백만개의 이미지-텍스트 페어를 4개의 A100으로 10시간 훈련. 이 단계만으로는 Vicuna가 이미지를 이해하지만, 토큰을 반복한다던가 사람이 원하는 형태로 이야기 하지 않았다.
  - 2단계
    - gpt3를 instruction dataset을 사용하여 gpt3.5로 발전시킨 것처럼 강화학습은 아니지만 요즘 많이 사용하는 instruction format을 사용했다.
    - 문제를 해결하고 사용성을 개선하기 위해, 모델 자체와 ChatGPT를 함께 이용해서 고품질 이미지-텍스트 쌍을 생성했다.
      - 1단계로 학습한 모델에게 5000쌍을 다음과 같이 인퍼런스 하도록 했다.
        - 아래와 같은 prompt 템플릿을 사용했다. <ImageFeature>는 BLIP-2(Q-former, ViT)와 linear layer를 통과한 임베딩 정보이다.

            ```
            ###Human:
            <Img><ImageFeature></Img> Describe this image in detail.
            Give as many details aspossible. Say everything you see.
            ###Assistant:
            ```

          - 그리고 만약 생성된 토큰이 80을 넘지 않는다면 아래와 같이 prompt를 다시 한번 주고 output을 concat 했다고 한다.

            ```
            ###Human: Continue 
            ###Assistant:
            ```

      - 그 결과를 가지고 ChatGPT에게 다음과 같은 prompt를 주고 수정하라고 했다.

        ```
        Fix the error in the given paragraph. Remove any repeating sentences, meaningless characters, not
        English sentences, and so on. Remove unnecessary repetition. Rewrite any incomplete sentences.
        Return directly the results without explanation. Return directly the input paragraph if it is already
        correct without explanation.
        ```

      - ChatGPT가 수정하지 못한 것들과 잘못된 묘사를 인간이 직접 검수하여 작은 규모(총 3500쌍)의 고품질 데이터 셋을 생성했다.
  
    - 2번째 파인튜닝 단계에서는 위에 만든 데이터셋으로 학습시켜서 생성에 대한 신뢰성과 전반적인 사용성을 개선했다.
      - 아래와 같은 prompt를 사용하고 <Instuction>에는 pre defined 된 “Describe this image in detail” or “Could you describe the contents of this image for me” 같은 것들을 사용했다고 한다.

        ```
        ###Human: <Img><ImageFeature></Img> <Instruction> 
        ###Assistant:
        ```

      - 다음과 같이 사용한 것 같다.

        ```
        <Img><ImageHere></Img> Describe this image in detail.
        <Img><ImageHere></Img> Take a look at this image and describe what you notice.
        <Img><ImageHere></Img> Please provide a detailed description of the picture.
        <Img><ImageHere></Img> Could you describe the contents of this image for me?
        ```

      - 이 단계는 계산 효율이 높아서 A100 한 대로 7분밖에 걸리지 않았다고 한다.

## 느낀점

- 이미지에 대한 임베딩 (BLIP-2(Q-former, ViT)와 linear layer를 통과한 임베딩 정보)을 prompt에 같이 넣고 text를 생성했다는 점이 신기했다. (두 모델을 앙상블하여 생성)

- 음성인식 분야에도 적용한다면 wav2vec conformer output의 정보를 주고 LLM을 다양한 instruction으로 학습한다면 일반적인 전사 뿐만 아니라 원하는 텍스트 스타일, 원하는 도메인에 맞게 더 전사를 잘 할 수 있지 않을까 생각이 든다.

- Vision 모델과 LLM을 leverage하고 linear layer 하나만 학습하였는데 잘 연결되어 학습이 되었다는 점도 좋았다.

- 실제 코드에서 학습하는 파라미터

    ```python
    self.llama_proj = nn.Linear(
        self.Qformer.config.hidden_size, self.llama_model.config.hidden_size
    )
    ```
