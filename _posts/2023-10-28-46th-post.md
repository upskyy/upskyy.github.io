---
title:  "Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation"
excerpt: ""
toc: true
toc_sticky: true

categories:
  - asr
tags:
  - asr
last_modified_at: 2023-10-28-08:00
---

[Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation](https://arxiv.org/abs/2309.08876)

<br>
<br>

## ABSTRACT

- encoder feature인 audio 정보를 ctc prediction을 통해서 decoder의 prompt로 사용합니다.
- 그러면 decoder의 구조는 autoregressive LM과 같은 구조가 되기 때문에 음성보다 데이터가 많은 텍스트 데이터로 다양한 도메인에 대해서 LM training을 할 수 있습니다.

<br>
<br>

## CTC PROMPTS FOR DECODER-ONLY ASR

<img width="477" alt="image" src="https://github.com/upskyy/upskyy.github.io/assets/54731898/92786de4-9eab-48cb-ac53-4bf6c739d7de">

<br>

- 일반적인 encoder-decoder 구조에서는 cross attention으로 인코더의 feature를 decoder에 전달했다면, 해당 논문에서는 ctc prediction을 하고  blank가 아닌 frame의 embedding 정보를 디코더의 prompt로 사용해서 학습했습니다.
  - 예를 들어, <aud> 토큰의 임베딩, (blank가 아닌 각 frame의 audio feature 임베딩들), <sos>의 임베딩, y1, y2, y3 ..
<br>

- 이렇게 학습했을 때 초반 학습에서 blank가 거의 나오지 않아 prompt 길이가 너무 길어지는 문제가 발생한다고 합니다. 이러한 ctc prediction의 immature 함을 확인하기 위해 tunable threshold 파라미터 θ를 따로 사용했다고 합니다.
<br>

- 이렇게 학습하면 decoder를 LM처럼 사용할 수 있기 때문에 LM 추가학습을 쉽게 할 수 있고, 다양한 도메인 데이터로 contextual bias도 진행할 수 있습니다.
<br>

- cross attention이 없어서 기존 encoder-decoder 모델보다 RTFx가 두배 정도 빠르다고 합니다.

<br>
<br>

## Training

### ASR Training

- [Hybrid CTC/Attention Architecture for End-to-End Speech Recognition](https://www.merl.com/publications/docs/TR2017-190.pdf) 논문에서 제안한 것과 동일하게 ctc loss과 decoder cross entropy loss를 더해서 학습했습니다.
<br>
- 위의 사진과 동일하게 ctc prediction을 통해서 prompt를 만들고 teacher forcing으로 ground truth를 input으로 넣어준 뒤, decoder의 cross entropy loss를 계산했습니다. 논문에서는 ctc loss weight를 0.3, decoder loss weight를 0.7로 설정했다고 합니다.
<br>
- 위에서 설명드린 tunable 파라미터 θ 값은 2로 설정하여, audio 정보의 prompt 길이가 텍스트의 길이의 2배를 넘는다면 immature 하다고 판단하여 해당 데이터셋은 audio prompt를 사용하지 않고, 그 자리에 ground truth 텍스트 데이터를 넣어서 학습했다고 합니다. (뒤에 설명할 LM training 방법을 사용한 것 입니다.)
<br>

### LM training

<img width="474" alt="image" src="https://github.com/upskyy/upskyy.github.io/assets/54731898/d9b4412c-fb6f-46b5-8f93-0396eeb5cb41">

<br>

- (a)는 정석적인 LM training 방법이고, (b)는 논문에서 제안한 LM training 방법입니다.
<br>
- LM 학습을 할 때 똑같은 데이터를 prompt로 넣으니까 모델이 그냥 외워서 뱉는 쉬운 task가 될 수 있기 때문에 앞에 prompt를 넣은 것과 넣지 않은 것을 1:1로 하여 LM 데이터셋을 만들었다고 합니다.
<br>
- 위에서 설명드린 immature 할 때 ground truth 텍스트 데이터를 prompt로 넣어서 학습했다고 한 내용이 (b)의 LM training 방법입니다.

<br>
<br>

## EXPERIMENTS

<img width="1032" alt="image" src="https://github.com/upskyy/upskyy.github.io/assets/54731898/8649f899-000d-4fe3-9c43-41f46be14967">

<br>
audio feature를 prompt로 만들 때, ctc prediction 결과를 convolution으로 downsampling도 해보고, 각 프레임을 모두 평균내서도 해보고, blank를 제거하고 텍스트의 임베딩 정보만 사용하는 방법도 해봤다고 합니다.
<br>
결과적으로 마지막 방법인 CTC remove 방법이 가장 좋은 결과가 나왔다고 합니다.
<br>
RTF를 보시면 B4인 Baseline EncDec 모델보다 논문에서 제안한 D3 모델이 약 2배정도 빠른 것을 확인 할 수 있습니다.

<br>
<br>

## 정리

- Meta에서 7월에 공개한 비슷한 논문 [Prompting Large Language Models with Speech Recognition Abilities](https://arxiv.org/pdf/2307.11795.pdf) 에서는 conformer encoder의 정보를 앞에서부터 3개의 frame씩 stack 하여 3배 압축을 하고 decoder의 prompt로 사용했는데, 본 논문에서는 ctc prediction을 통해서 prompt를 만들었다는 점이 특별한 것 같습니다.
- 하지만 학습할 때 ctc prediction의 dependency가 크기 때문에 초반에 blank가 잘 나오도록 학습하는 것이 중요할 것 같고, 그렇기 때문에 꽤 학습이 불안정할 것 같다고 생각합니다.
- 이러한 불안정함을 해소하기 위해 ctc prediction은 ctc loss를 계산하는데 사용하고, conformer encoder의 output에 linear layer를 하나 적용하여 prompt로 사용하는 방법도 있을 것 같습니다.
- 하지만 이 방법을 사용하면 audio feature의 T를 줄일 수 없기 때문에 적절하게 T를 줄이는 것이 중요할 것 같습니다.
- 음성은 아니지만, 비슷한 방법으로 이미지와 텍스트를 연결한 [MiniGPT-4](https://upskyy.github.io/llm/40th-post/)도 참고해볼 수 있을 것 같습니다.

<br>
